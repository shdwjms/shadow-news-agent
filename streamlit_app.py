# ‚úÖ Shadow Intel Agent - Streamlit App (Fixed Version)

import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import random

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–∞–Ω–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞
st.set_page_config(
    page_title="Shadow Intel Agent",
    page_icon="üïµÔ∏è",
    layout="wide"
)

st.title("üïµÔ∏è Shadow Intel Agent")
st.subheader("üì∞ AI & Tech Intelligence Gathering")

# –ò–∑–±–æ—Ä –Ω–∞ —Ç–µ–º–∞
query = st.selectbox(
    "üì∞ –ò–∑–±–µ—Ä–∏ —Ç–µ–º–∞, –∫–æ—è—Ç–æ —Ç–µ –∏–Ω—Ç–µ—Ä–µ—Å—É–≤–∞:",
    ["AI investing", "AI market", "AI stocks", "Nvidia", "OpenAI", "ChatGPT", "Tesla AI"]
)

# –ë—É—Ç–æ–Ω –∑–∞ —Ç—ä—Ä—Å–µ–Ω–µ
if st.button("üîç –ó–∞–ø–æ—á–Ω–∏ —Ç—ä—Ä—Å–µ–Ω–µ—Ç–æ"):
    with st.spinner("–°—ä–±–∏—Ä–∞–º —Ä–∞–∑—É–∑–Ω–∞–≤–∞—Ç–µ–ª–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è... üïµÔ∏è"):
        try:
            # –ü–æ-–¥–æ–±—Ä–∏ headers –∑–∞ –∏–∑–±—è–≥–≤–∞–Ω–µ –Ω–∞ –±–ª–æ–∫–∏—Ä–∞–Ω–µ
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive"
            }
            
            # –ê–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–µ–Ω –ø–æ–¥—Ö–æ–¥ - –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ DuckDuckGo –≤–º–µ—Å—Ç–æ Google
            search_url = f"https://duckduckgo.com/html/?q={query}+site:reuters.com+OR+site:finance.yahoo.com+OR+site:marketwatch.com"
            
            # –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ –∑–∞–∫—ä—Å–Ω–µ–Ω–∏–µ
            time.sleep(random.uniform(1, 3))
            
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            results = []
            
            # –¢—ä—Ä—Å–µ–Ω–µ –∑–∞ DuckDuckGo —Ä–µ–∑—É–ª—Ç–∞—Ç–∏
            for result in soup.find_all('div', class_='result'):
                title_elem = result.find('h2')
                link_elem = result.find('a', class_='result__url')
                snippet_elem = result.find('div', class_='result__snippet')
                
                if title_elem and link_elem:
                    title = title_elem.get_text().strip()
                    link = link_elem.get('href', '')
                    snippet = snippet_elem.get_text().strip() if snippet_elem else "–ù—è–º–∞ –æ–ø–∏—Å–∞–Ω–∏–µ"
                    
                    results.append({
                        "title": title,
                        "link": link,
                        "snippet": snippet
                    })
            
            # –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ
            if results:
                st.success(f"‚úÖ –ù–∞–º–µ—Ä–µ–Ω–∏ —Å–∞ {len(results)} —Ä–µ–∑—É–ª—Ç–∞—Ç–∞!")
                
                for i, result in enumerate(results[:10]):  # –ü–æ–∫–∞–∑–≤–∞–º–µ —Å–∞–º–æ –ø—ä—Ä–≤–∏—Ç–µ 10
                    with st.expander(f"üì∞ {result['title'][:80]}..."):
                        st.write(f"**–ó–∞–≥–ª–∞–≤–∏–µ:** {result['title']}")
                        st.write(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {result['snippet']}")
                        st.write(f"**–õ–∏–Ω–∫:** [{result['link']}]({result['link']})")
                        st.divider()
            else:
                st.warning("‚ö†Ô∏è –ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏. –û–ø–∏—Ç–∞–π —Å –¥—Ä—É–≥–∞ –∫–ª—é—á–æ–≤–∞ –¥—É–º–∞.")
                st.info("üí° **–°—ä–≤–µ—Ç:** –í—ä–∑–º–æ–∂–Ω–æ –µ —Å–∞–π—Ç—ä—Ç –¥–∞ –±–ª–æ–∫–∏—Ä–∞ –∑–∞—è–≤–∫–∏—Ç–µ. –û–ø–∏—Ç–∞–π —Å–ª–µ–¥ –º–∞–ª–∫–æ.")
                
        except requests.exceptions.RequestException as e:
            st.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—è–≤–∫–∞—Ç–∞: {str(e)}")
            st.info("üí° **–ê–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** –û–ø–∏—Ç–∞–π –¥–∏—Ä–µ–∫—Ç–Ω–æ –≤ –±—Ä–∞—É–∑—ä—Ä–∞ –∏–ª–∏ –∏–∑–ø–æ–ª–∑–≤–∞–π VPN.")
            
        except Exception as e:
            st.error(f"‚ùå –ù–µ–æ—á–∞–∫–≤–∞–Ω–∞ –≥—Ä–µ—à–∫–∞: {str(e)}")

# –ê–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–µ–Ω —Ä–∞–∑–¥–µ–ª —Å –¥–∏—Ä–µ–∫—Ç–Ω–∏ –ª–∏–Ω–∫–æ–≤–µ
st.divider()
st.subheader("üîó –î–∏—Ä–µ–∫—Ç–Ω–∏ –∏–∑—Ç–æ—á–Ω–∏—Ü–∏")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("### üìä –§–∏–Ω–∞–Ω—Å–∏")
    st.markdown("- [Yahoo Finance](https://finance.yahoo.com)")
    st.markdown("- [MarketWatch](https://www.marketwatch.com)")
    st.markdown("- [Reuters](https://www.reuters.com)")

with col2:
    st.markdown("### ü§ñ AI –ù–æ–≤–∏–Ω–∏")
    st.markdown("- [VentureBeat AI](https://venturebeat.com/ai/)")
    st.markdown("- [The Verge AI](https://www.theverge.com/ai-artificial-intelligence)")
    st.markdown("- [AI News](https://www.artificialintelligence-news.com/)")

with col3:
    st.markdown("### üíº –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏")
    st.markdown("- [Seeking Alpha](https://seekingalpha.com)")
    st.markdown("- [TechCrunch](https://techcrunch.com)")
    st.markdown("- [The Information](https://www.theinformation.com)")

# Footer
st.divider()
st.caption("Made with ‚òï and sarcasm by Shadow & James | üî• Powered by Streamlit")

# –°—Ç—Ä–∞–Ω–∏—á–Ω–∞ –ª–µ–Ω—Ç–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
with st.sidebar:
    st.header("‚ÑπÔ∏è –ö–∞–∫ —Ä–∞–±–æ—Ç–∏?")
    st.write("""
    1. üéØ –ò–∑–±–µ—Ä–∏ —Ç–µ–º–∞ –æ—Ç –º–µ–Ω—é—Ç–æ
    2. üîç –ù–∞—Ç–∏—Å–Ω–∏ '–ó–∞–ø–æ—á–Ω–∏ —Ç—ä—Ä—Å–µ–Ω–µ—Ç–æ'
    3. üìä –†–∞–∑–≥–ª–µ–¥–∞–π —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ
    4. üîó –ò–∑–ø–æ–ª–∑–≤–∞–π –¥–∏—Ä–µ–∫—Ç–Ω–∏—Ç–µ –ª–∏–Ω–∫–æ–≤–µ –∑–∞ –ø–æ–≤–µ—á–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    """)
    
    st.header("‚ö†Ô∏è –ó–∞–±–µ–ª–µ–∂–∫–∏")
    st.write("""
    - –ù—è–∫–æ–∏ —Å–∞–π—Ç–æ–≤–µ –º–æ–≥–∞—Ç –¥–∞ –±–ª–æ–∫–∏—Ä–∞—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–∞–Ω–∏ –∑–∞—è–≤–∫–∏
    - –ó–∞ –Ω–∞–π-–∞–∫—Ç—É–∞–ª–Ω–∏ –¥–∞–Ω–Ω–∏ –∏–∑–ø–æ–ª–∑–≤–∞–π –¥–∏—Ä–µ–∫—Ç–Ω–∏—Ç–µ –ª–∏–Ω–∫–æ–≤–µ
    - –ê–∫–æ –∏–º–∞ –≥—Ä–µ—à–∫–∏, –æ–ø–∏—Ç–∞–π —Å–ª–µ–¥ –Ω—è–∫–æ–ª–∫–æ –º–∏–Ω—É—Ç–∏
    """)
